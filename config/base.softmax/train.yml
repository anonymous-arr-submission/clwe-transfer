accum_count:
- 6
adam_beta2: 0.999
batch_size: 5120
batch_type: tokens
data_weights:
- 1
decay_method: noam
decoder_type: transformer
dropout:
- 0.2
early_stopping: 5
encoder_type: transformer
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
heads: 6
keep_checkpoint: 8
label_smoothing: 0.1
layers: 9
learning_rate: 5.0
learning_rate_decay: 0.5
loss_scale: 0
max_generator_batches: 2
max_grad_norm: 25
min_lr: 1.0e-09
model_dtype: fp32
model_type: text
normalization: tokens
optim: adam
param_init: 0.0
param_init_glorot: 'true'
pool_factor: 8192
position_encoding: 'true'
report_every: 250
#reset_optim: all
rnn_size: 300
save_checkpoint_steps: 1000
self_attn_type: scaled-dot
share_decoder_embeddings: 'true'
share_embeddings: 'true'
train_steps: 160000
transformer_ff: 1200
truncated_decoder: 0
valid_batch_size: 8
valid_steps: 1000
warmup_end_lr: 0.0007
warmup_init_lr: 1.0e-08
warmup_steps: 4000
word_vec_size: 300
detached_embeddings: 'true'
pre_word_vecs_enc: 'saves/base/embeddings.tgt.pt'
pre_word_vecs_dec: 'saves/base/embeddings.tgt.pt'
#no_generator_bias: 'true'
#world_size: 2
world_size: 1
master_port: 10001
