accum_count:
- 2
accum_steps:
- 0
adagrad_accumulator_init: 0
adam_beta1: 0.9
adam_beta2: 0.9995
alignment_layer: -3
apex_opt_level: O1
attention_dropout:
- 0.1
audio_enc_pooling: '1'
average_decay: 0
average_every: 1
batch_size: 8192
batch_type: tokens
cnn_kernel_width: 3
data_weights:
- 1
dec_layers: 2
dec_rnn_size: 500
decay_method: linear
decay_steps: 10000
decoder_type: transformer
dropout:
- 0.1
dropout_steps:
- 0
early_stopping: 0
enc_layers: 2
enc_rnn_size: 500
encoder_type: transformer
epochs: 0
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: continuous-linear
generator_layer_norm: 'true'
global_attention: general
global_attention_function: softmax
gpu_backend: nccl
gpu_ranks:
- 0
gpu_verbose_level: 0
gpuid: []
heads: 6
image_channel_size: 3
input_feed: 1
keep_checkpoint: -1
label_smoothing: 0.1
lambda_align: 0.0
lambda_coverage: 0.0
lambda_mtl: 1.0
lambda_vmf: 0.2
layers: 6
learning_rate: 1.0
learning_rate_decay: 0.5
log_file_level: '0'
loss: nllvmf
loss_scale: 0
master_port: 10000
max_generator_batches: 2
max_grad_norm: 5.0
max_relative_positions: 0
min_lr: 1.0e-09
model_dtype: fp32
model_type: text
normalization: tokens
optim: radam
param_init: 0.0
param_init_glorot: 'true'
pool_factor: 8192
position_encoding: 'true'
queue_size: 400
report_every: 250
reset_optim: none
rnn_size: 300
rnn_type: LSTM
sample_rate: 16000
save_checkpoint_steps: 10000
seed: -1
self_attn_type: scaled-dot
share_decoder_embeddings: 'true'
share_embeddings: 'true'
src_word_vec_size: 500
start_decay_steps: 50000
tgt_word_vec_size: 500
train_steps: 90000
transformer_ff: 1200
truncated_decoder: 0
valid_batch_size: 32
valid_steps: 10000
warmup_end_lr: 0.0007
warmup_init_lr: 1.0e-08
warmup_steps: 4000
weight_decay: 1.0e-05
window_size: 0.02
word_vec_size: 300
world_size: 1
