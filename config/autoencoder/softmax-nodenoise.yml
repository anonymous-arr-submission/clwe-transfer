accum_count:
- 2
adam_beta2: 0.9995
#batch_size: 65536
#batch_size: 16384
batch_size: 10240
batch_type: tokens
decay_method: linear
dropout:
- 0.2
early_stopping: 1
keep_checkpoint: 5
label_smoothing: 0.1
learning_rate: 1.0
learning_rate_decay: 0.5
loss_scale: 0
max_generator_batches: 2
max_grad_norm: 25
min_lr: 1.0e-09
normalization: tokens
optim: adam
report_every: 50
reset_optim: all
save_checkpoint_steps: 200
share_decoder_embeddings: 'true'
share_embeddings: 'true'
train_steps: 600
truncated_decoder: 0
valid_steps: 100
valid_batch_size: 4
warmup_end_lr: 0.0007
warmup_init_lr: 1.0e-08
warmup_steps: 500
weight_decay: 1.0e-05
modify_opts: 'true'
denoise: 'false'
model_dtype: fp32
freeze_encoder: 'true'
fix_word_vecs_enc: 'true'
fix_word_vecs_dec: 'true'
