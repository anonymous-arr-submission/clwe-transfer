accum_count:
- 1
adam_beta2: 0.9995
#batch_size: 16384
#batch_size: 32768
batch_size: 49152
batch_type: tokens
decay_method: linear
dropout:
- 0.1
#early_stopping: 10
early_stopping: 3
early_stopping_criteria: continuous
keep_checkpoint: 6
label_smoothing: 0.1
lambda_vmf: 0.2
learning_rate: 1.0
learning_rate_decay: 0.5
loss: nllvmf
loss_scale: 0
max_generator_batches: 2
max_grad_norm: 5.0
min_lr: 1.0e-09
normalization: tokens
optim: radam
report_every: 50
reset_optim: all
save_checkpoint_steps: 250
share_decoder_embeddings: 'true'
train_steps: 12000
truncated_decoder: 0
valid_steps: 250
warmup_end_lr: 0.0007
warmup_init_lr: 1.0e-08
warmup_steps: 2000
weight_decay: 1.0e-05
modify_opts: 'true'
model_dtype: fp16
fix_word_vecs_enc: 'true'
fix_word_vecs_dec: 'true'
#freeze_encoder: 'true'
#use_feat_emb: 'true'
use_feat_emb: 'false'
#feat_vec_size: 6
